{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LELA60331 Week 5 Seminar Workbook\n",
        "\n",
        "This week we will look at regression and at model training using gradient descent.\n",
        "\n",
        "We will start by looking at fitting linear regression models."
      ],
      "metadata": {
        "id": "k89xlsM2pooY"
      },
      "id": "k89xlsM2pooY"
    },
    {
      "cell_type": "markdown",
      "id": "25f8ebaa",
      "metadata": {
        "id": "25f8ebaa"
      },
      "source": [
        "### Linear regression with one predictor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this (made up) data set y is the exam results of a group of students and x is the number of hours spent studying."
      ],
      "metadata": {
        "id": "9CPkssfF_mpI"
      },
      "id": "9CPkssfF_mpI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d7882cd",
      "metadata": {
        "id": "3d7882cd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "x = [1.00,1.25,1.50,1.75,2.00,2.25,2.50,2.75,3.00,3.25,3.50,3.75,4.00]\n",
        "y = [33,49,41,54,52,45,36,58,45,69,55,56,68]\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.ylim(0,100)\n",
        "plt.xlim(0,5)\n",
        "plt.scatter(x, y)\n",
        "fdsfds\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In linear regression we fit a line to the data that allows us to predict y from x. The equation for this is as follows (note that I using the  machine-learning term bias in place of the term intercept which is more commonly used in statistics).\n",
        "\n",
        "y = bias + x*weight [+ error]\n",
        "\n",
        "Let's start by setting a random weight and setting our bias to zero.\n",
        "\n",
        "Note: Computers cannot actually generate random numbers, just numbers that appear random. So we would do better to consider the numbers generated to be pseudo-random. The algorithm deployed uses a \"seed\" during generation. If the seed is held constant then we will get the same \"random\" value each time. I set the seed here just so that everyone gets the same line and things don't get confusing. If I hadn't set this I would expect a different weight everytime I ran the code.\n"
      ],
      "metadata": {
        "id": "aO8EzOQ8DATd"
      },
      "id": "aO8EzOQ8DATd"
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "weight = np.random.rand(1)[0]\n",
        "bias=0\n",
        "weight"
      ],
      "metadata": {
        "id": "zDFRmccDD0KM"
      },
      "id": "zDFRmccDD0KM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "683df270",
      "metadata": {
        "id": "683df270"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x, y)\n",
        "plt.ylim(0,100)\n",
        "plt.xlim(0,5)\n",
        "line_x = [0, 5]\n",
        "line_y = [bias, bias+(5*weight)]\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(line_x, line_y, label='Line', color='red')  # Adding a line\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see this line isn't a very good fit to the data. If we used it for predictions we would get very bad estimates.\n",
        "\n",
        "We will use gradient descent to fine the line of best fit. Note: it is called simply gradient descent when we update weights based on the whole data set as we will here. If we update weights based on random subsets of the data (mini-batch training) it is called stochastic gradient descent. These variants are explained in the week 7 lecture."
      ],
      "metadata": {
        "id": "WQRRYvlEG51y"
      },
      "id": "WQRRYvlEG51y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent works as follows: \\\n",
        "Initialise weights to e.g. random values \\\n",
        "For a given number of N iterations:  \n",
        "1. Predict each y value given x, bias and the current weight(s), giving us y_hat\n",
        "2. Calculate the loss (for reporting/monitoring). For linear regression this is the \"mean squared error\". We calculate a vector of item-specific \"errors\" by subtracting the vector of real y values from the vector of estimated y_hat values. We then calculate the dot product of this vector with itself (the sum of the squared values) and divide that by the number of data points.\n",
        "3. Calculate dw - the gradient of the loss function with regard to each weight. For linear regression this is the dot product of the vector of x values for the given feature and the vector of errors, divided by the number of data points in our data.\n",
        "4. Calculate db - the gradient of the loss function with regard to the bias. For linear regression this is the sum of the vector of errors, divided by the number of data points in our data.  \n",
        "5. Update each weight (in this first example there is only one) by setting it to be the current weight minus dw times the learning rate\n",
        "6. Update the bias by setting it to be the current bias minus db times the learning rate\n",
        "7. Repeat until done\n"
      ],
      "metadata": {
        "id": "0LjBqRrmkrXx"
      },
      "id": "0LjBqRrmkrXx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1: Complete the code below so that it finds the line of best fit"
      ],
      "metadata": {
        "id": "H2RcrtIVDISG"
      },
      "id": "H2RcrtIVDISG"
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 250\n",
        "num_features=1\n",
        "weight = np.random.rand(num_features)[0]\n",
        "bias=0\n",
        "linear_loss=[]\n",
        "\n",
        "num_samples = len(y)\n",
        "lr=0.01\n",
        "\n",
        "for i in range(n_iters):\n",
        "    y_est =\n",
        "    errors = ?\n",
        "    loss = ?\n",
        "    linear_loss.append(loss)\n",
        "\n",
        "    #dw = ?\n",
        "    #db = ?\n",
        "    #weight = ?\n",
        "    #bias = ?\n",
        "\n",
        "plt.plot(range(1,n_iters),linear_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "qyhjhn_hlV7J"
      },
      "id": "qyhjhn_hlV7J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we have done this correctly then when we use the bias and the weight to plot a line it should fit the data well"
      ],
      "metadata": {
        "id": "cfh6J7QFRZHM"
      },
      "id": "cfh6J7QFRZHM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06a5164c",
      "metadata": {
        "id": "06a5164c"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x, y)\n",
        "plt.ylim(0,100)\n",
        "plt.xlim(0,5)\n",
        "line_x = [0, 5]\n",
        "line_y = [bias, bias+(5*weight)]\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(line_x, line_y, label='Line', color='red')  # Adding a line\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2: Increase the learning rate to 2 and rerun your code. What do you notice? Why does this happen? \\\\\n"
      ],
      "metadata": {
        "id": "yAaIvbKIE_bt"
      },
      "id": "yAaIvbKIE_bt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3. Decrease the learning rate to 0.001 and rerun your code. What do you notice? Why does this happen?\n"
      ],
      "metadata": {
        "id": "y6a41WszFDqt"
      },
      "id": "y6a41WszFDqt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4. What is the predicted exam score for a student who studied for a) 1 hour and b) 4 hours. To calculate this you will need to know the bias and the weight which are as follows:\n"
      ],
      "metadata": {
        "id": "IuEzG8GTBYgt"
      },
      "id": "IuEzG8GTBYgt"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"BIAS: \" + str(bias))\n",
        "print(\"WEIGHT: \" + str(weight))"
      ],
      "metadata": {
        "id": "PghW0z1cSPZ2"
      },
      "id": "PghW0z1cSPZ2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression with 2 predictors\n",
        "\n",
        "Now imagine that we acquire an additional piece of information - each student's score on a recent mock exam. We now have two features to use to make our prediction. We can plot the relationship between the variables using a bubble plot."
      ],
      "metadata": {
        "id": "eg0gn4eJzEfb"
      },
      "id": "eg0gn4eJzEfb"
    },
    {
      "cell_type": "code",
      "source": [
        "x=[[41, 51, 35, 45, 52, 35, 31, 57, 45, 51, 60, 64, 63],[1.00,1.25,1.50,1.75,2.00,2.25,2.50,2.75,3.00,3.25,3.50,3.75,4.00]]\n",
        "y = [33,49,41,54,52,45,36,58,45,69,55,56,68]\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "plt.scatter(x[0], x[1], s=np.exp(y/10), alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NL3dWZH93-Bz"
      },
      "id": "NL3dWZH93-Bz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before attempting to do anything with the data we will scale the predictors so that they have a mean of zero and a standard deviation of 1. This will aid learning as weights will be on manageable scales.\n"
      ],
      "metadata": {
        "id": "lebYsFJ3w11w"
      },
      "id": "lebYsFJ3w11w"
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]=(x[0]-np.mean(x[0]))/np.std(x[0])\n",
        "x[1]=(x[1]-np.mean(x[1]))/np.std(x[1])"
      ],
      "metadata": {
        "id": "yM-GZEXExLS_"
      },
      "id": "yM-GZEXExLS_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 5: Complete the code below so that it finds the line of best fit. The critical difference is that we now have 2 features and therefore 2 weights to factor into our predictions and to update at each iteration"
      ],
      "metadata": {
        "id": "tNRtwWudlsJb"
      },
      "id": "tNRtwWudlsJb"
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 2000\n",
        "num_features = 2\n",
        "weights = np.random.rand(num_features)\n",
        "bias=0\n",
        "num_samples = len(y)\n",
        "linear_loss=[]\n",
        "lr=0.025\n",
        "for i in range(n_iters):\n",
        "    y_est =\n",
        "    errors = ?\n",
        "    loss = ?\n",
        "    linear_loss.append(loss)\n",
        "\n",
        "    dw1 = ?\n",
        "    dw2 = ?\n",
        "    db = ?\n",
        "    weights[0] = ?\n",
        "    weights[1] = ?\n",
        "    bias = bias - lr * db\n",
        "plt.plot(range(1,n_iters),linear_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "eRzZhdcWlvl5"
      },
      "id": "eRzZhdcWlvl5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3e70908"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "Next we are going to apply what we have learned to logistic regression\n",
        "\n",
        "First we will generate some random data for an imagined sentiment classification task with only two features. We can think of our two features as being the log of the counts of positive words (e.g. good, excellent) and the log of the counts of negative words (e.g. bad, rubbish). The label we are trying to predict is either 1 (positive sentiment text) or 0 (negative sentiment text)."
      ],
      "id": "d3e70908"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "RRsFYQMuzbvB"
      },
      "execution_count": null,
      "outputs": [],
      "id": "RRsFYQMuzbvB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75fd7147"
      },
      "outputs": [],
      "source": [
        "## Create simulated data\n",
        "np.random.seed(10)\n",
        "w1_center = (2, 3)\n",
        "w2_center = (3, 2)\n",
        "batch_size=50\n",
        "\n",
        "x = np.zeros((batch_size, 2))\n",
        "y = np.zeros(batch_size)\n",
        "for i in range(batch_size):\n",
        "    if np.random.random() > 0.5:\n",
        "        x[i] = np.random.normal(loc=w1_center)\n",
        "    else:\n",
        "        x[i] = np.random.normal(loc=w2_center)\n",
        "        y[i] = 1\n",
        "\n",
        "x=x.T"
      ],
      "id": "75fd7147"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualise the data as follows. The stars are the positive sentiment texts, the circles are the negative sentiment texts."
      ],
      "metadata": {
        "id": "hrt8YVTM4rb2"
      },
      "id": "hrt8YVTM4rb2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32bd6db8"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x[0][y==0], x[1][y==0], marker='*', s=100)\n",
        "plt.scatter(x[0][y==1], x[1][y==1], marker='o', s=100)\n",
        "plt.xlabel(\"log count of negative words\")\n",
        "plt.ylabel(\"log count of positive words\")\n",
        "plt.xlim((0,5))\n",
        "plt.ylim((0,5))\n"
      ],
      "id": "32bd6db8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see why we might to take the log, we can exponentiate the log counts (reversing the log function) to give raw counts. These are worse for visualisation and modelling purposes"
      ],
      "metadata": {
        "id": "T-bHfx_w42y3"
      },
      "id": "T-bHfx_w42y3"
    },
    {
      "cell_type": "code",
      "source": [
        "x_exp=np.exp(x)\n",
        "plt.scatter(x_exp[0][y==0], x_exp[1][y==0], marker='*', s=100)\n",
        "plt.scatter(x_exp[0][y==1], x_exp[1][y==1], marker='o', s=100)\n",
        "plt.xlabel(\"count of negative words\")\n",
        "plt.ylabel(\"count of positive words\")\n",
        "plt.xlim((0,150))\n",
        "plt.ylim((0,150))"
      ],
      "metadata": {
        "id": "f35tC8HF0k-r"
      },
      "execution_count": null,
      "outputs": [],
      "id": "f35tC8HF0k-r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal in logistic regression is to find a line that allows us to estimate a probability that any text has positive sentiment. It that probability is greater than 0.5 then we will say that it is a positive text and if lower then we will say it is a negative text.\n",
        "\n",
        "In logistic regression we first estimate a value z as a linear function of our predictors, just as in linear regression:\n",
        "\n",
        "$y_{i} = bias + x1_{i}*weight1 + x2_{i}*weight2$\n",
        "\n",
        "We then use the sigmoid function to convert this z values to a probability:\n",
        "\n",
        "$p(y_{i}=1) = \\frac{1}{1+e^{-z}}$\n",
        "\n",
        "\n",
        "We can start by setting some random weights and an arbitrary bias."
      ],
      "metadata": {
        "id": "4O3GYaLW5TXk"
      },
      "id": "4O3GYaLW5TXk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1078b2a"
      },
      "outputs": [],
      "source": [
        "np.random.seed(10)\n",
        "num_features=2\n",
        "weights = np.random.rand(num_features)\n",
        "bias=0"
      ],
      "id": "b1078b2a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can add this line to our plot of values. It should cut across the items so that items that are above the line should be mostly positive sentiment texts and those that are below should be negative sentiment texts."
      ],
      "metadata": {
        "id": "a-y6Y6m96klo"
      },
      "id": "a-y6Y6m96klo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a287cb21"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x[0][y==0], x[1][y==0], marker='*', s=100)\n",
        "plt.scatter(x[0][y==1], x[1][y==1], marker='o', s=100)\n",
        "plt.xlim((-5,5))\n",
        "plt.ylim((-5,5))\n",
        "c = -bias/weights[1]\n",
        "m = -weights[0]/weights[1]\n",
        "xmin, xmax = 0, 5\n",
        "ymin, ymax = 0, 5\n",
        "xd = np.array([xmin, xmax])\n",
        "yd = m*xd + c\n",
        "plt.plot(xd, yd, 'k', lw=1, ls='--')"
      ],
      "id": "a287cb21"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our random line does not do this. So we will use gradient descent to find the line of best fit.\n",
        "\n",
        "For logistic regression we use a cross entropy loss function. I have included this in the code (See lecture for details).\n",
        "\n",
        "To calculate the gradient of the loss function with respect to the bias term we first calculate the difference between each predicted y value and the true y value. We then take the average difference by summing the differences and dividing the result by N - the number of data points in our data:\n",
        "\n",
        "$\\delta b = \\frac{1}{N} * \\sum_{i=1}^{N} q_{i} - y_{i} $\n",
        "\n",
        "To calculate the gradient of the loss function with respect to each weight, we again first calculate the difference between each predicted y value and the true y value. We then calculate the dot product of this vector and the vector of x values for the relevant feature and divide the result by N - the number of data points in our data:\n",
        "\n",
        "$\\delta w = \\frac{1}{N} * \\sum_{i=1}^{N} x[i] * q_{i} - y_{i} $\n",
        "\n",
        "x here is vector of values for the feature relevant to the individual weight. A different gradient is needed for each weight and this will be calculated using a different x.\n"
      ],
      "metadata": {
        "id": "dskOZYTy6_8N"
      },
      "id": "dskOZYTy6_8N"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 6: Complete code below so that it finds the line of best fit. \\\\\n",
        "\n",
        "Note: For the sigmoid function you will need to exponentiate -z. You can do this using the function np.exp(-z)."
      ],
      "metadata": {
        "id": "gxnf8Ob5mWyb"
      },
      "id": "gxnf8Ob5mWyb"
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "num_features=2\n",
        "weights = np.random.rand(num_features)\n",
        "bias=0\n",
        "\n",
        "n_iters = 2500\n",
        "num_features = 2\n",
        "num_samples = len(y)\n",
        "lr=0.001\n",
        "logistic_loss=[]\n",
        "\n",
        "for i in range(n_iters):\n",
        "    z=????\n",
        "    q = ????\n",
        "    loss = sum(-(y*np.log2(q)+(1-y)*np.log2(1-q)))\n",
        "    logistic_loss.append(loss)\n",
        "    dw1 = ?????\n",
        "    dw2 = ?????\n",
        "    db = ?????\n",
        "    weights[0] = ?????\n",
        "    weights[1] = ?????\n",
        "    bias = ?????\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "m62X7TZn5gGi"
      },
      "execution_count": null,
      "outputs": [],
      "id": "m62X7TZn5gGi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once this is working we can add the resulting line to our data and it should separate the two classes of items."
      ],
      "metadata": {
        "id": "mkZ8zmaBDMBQ"
      },
      "id": "mkZ8zmaBDMBQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1414def9"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x[0][y==0], x[1][y==0], marker='*', s=100)\n",
        "plt.scatter(x[0][y==1], x[1][y==1], marker='o', s=100)\n",
        "plt.xlim((-5,5))\n",
        "plt.ylim((-5,5))\n",
        "c = -bias/weights[1]\n",
        "m = -weights[0]/weights[1]\n",
        "xmin, xmax = 0, 5\n",
        "ymin, ymax = -5, 5\n",
        "xd = np.array([xmin, xmax])\n",
        "yd = m*xd + c\n",
        "plt.plot(xd, yd, 'k', lw=1, ls='--')"
      ],
      "id": "1414def9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 7: Calculate p(y=1) for a) a text that contains two positive words and 3 negative words and b) a text that contains 10 positive words and 1 negative word.\n",
        "\n",
        "To calculate this you will need to know the bias and the weight which are as follows. You will also need to use the sigmoid function."
      ],
      "metadata": {
        "id": "cZ1qxyrlDd6_"
      },
      "id": "cZ1qxyrlDd6_"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"BIAS: \" + str(bias))\n",
        "print(\"WEIGHT 1: \" + str(weights[0]))\n",
        "print(\"WEIGHT 2: \" + str(weights[1]))"
      ],
      "metadata": {
        "id": "f2MMXcb8D1hS"
      },
      "execution_count": null,
      "outputs": [],
      "id": "f2MMXcb8D1hS"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}