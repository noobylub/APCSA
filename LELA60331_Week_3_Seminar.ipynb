{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noobylub/APCSA/blob/master/LELA60331_Week_3_Seminar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvSE04uu_Pe0"
      },
      "source": [
        "# Week 3 Seminar Notebook\n",
        "This week we are going to be thinking about and working with n-gram language models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N7_AihvU3Uiw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x3yCQbgF-aE"
      },
      "source": [
        "# Lists\n",
        "\n",
        "You have heard in your research methods Python session about the different data types in Python and about various data structures. Here follows a brief refresh for anyone who needs one.\n",
        "\n",
        "\n",
        "The first data structure we have encountered is the list. I have used this term informally in previous CL1 sessions because the everyday English word list capture quite well what a Python list is: an ordered store of entities, where those entities can be e.g. numbers, letter, words, other lists. We represent it using square brackets, such that we would create of list of integers from 1 to 10 like this:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akK3qq-yHcNh"
      },
      "outputs": [],
      "source": [
        "nums=[1,2,3,4,5,6,7,8,9,10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH6YV6PdHzZe"
      },
      "source": [
        "We can represent the first five letters of the alphabet like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yqx00ezYH6I9"
      },
      "outputs": [],
      "source": [
        "alphabet=[\"a\",\"b\",\"c\",\"d\",\"e\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAa6oiXBIDoC"
      },
      "source": [
        "We can represent a sentence like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xO2OOTAUHgX_"
      },
      "outputs": [],
      "source": [
        "sentence = [\"this\", \"is\", \"a\", \"sentence\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9umW5JK1IKXp"
      },
      "source": [
        "And we can represent a series of sentences like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8oq0HR6IKmp"
      },
      "outputs": [],
      "source": [
        "sentences = [[\"this\", \"is\", \"a\", \"sentence\"],[\"this\",\"is\",\"another\",\"sentence\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku11ujdgJRJN"
      },
      "source": [
        "We can print the contents of a list as a single string. The character in the quotes before \".join\" sets the character to be printed between the elements of the list. Here we use a space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EWSCRztJWv6"
      },
      "outputs": [],
      "source": [
        "str.join(\" \", sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kGuJorOHgX_"
      },
      "source": [
        "We can also select elements from within the list. The entries in a list are indexed numberically starting with zero. So the first element is sentence[0] and the last element of this four element list is sentence[3]. These can then be select for printing as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69l83SK-HgYA"
      },
      "outputs": [],
      "source": [
        "sentence[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS2RC2T8HgYA"
      },
      "source": [
        "We can also select subsequences of entries, by specifying a range as follows. Notice that the second character in the range isn't included - so 0:2 means from 0 up to the number before 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo9ZgHryHgYA"
      },
      "outputs": [],
      "source": [
        "str.join(\" \", sentence[0:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciEFnb99HgYA"
      },
      "source": [
        "This allows us to, for example, insert elements in the middle of sentences as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn66kOmDHgYA"
      },
      "outputs": [],
      "source": [
        "print(str.join(\" \", sentence[0:3]) + \" short \" + sentence[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM8O3DHrHfeM"
      },
      "source": [
        "An important thing to note is that a string in Python is a list of characters. So that you can select character from within strings as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0Rn22l-Lira"
      },
      "outputs": [],
      "source": [
        "sentence2 = \"the dog lay on the rug\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDgdKw__LuNc"
      },
      "outputs": [],
      "source": [
        "sentence2[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjB0bTCV2h33"
      },
      "source": [
        "You can find the length of a list as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1YpuKdj2gaB"
      },
      "outputs": [],
      "source": [
        "len(sentence2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RssxlPiU2C8V"
      },
      "source": [
        "You have also briefly encountered for loops. You can iterate over the elements in a list as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGpLMLbc2HE_"
      },
      "outputs": [],
      "source": [
        "for elem in sentence2:\n",
        "    print(elem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erjcXH4t2dxN"
      },
      "source": [
        "Or by index as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ahy6EtB2U2u"
      },
      "outputs": [],
      "source": [
        "for i in range(len(sentence2)):\n",
        "    print(sentence2[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRhjvS71Mwk8"
      },
      "source": [
        "### Dictionaries\n",
        "A second useful data structure is the dictionary. This stores data in key and value pairs. There is a flexibility in the data types that can be keys and can be values, for example the former could be a string or an int. The latter could be a list or even another dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzZyc9SENAqp"
      },
      "outputs": [],
      "source": [
        "thisdict = {\n",
        "  \"brand\": \"Ford\",\n",
        "  \"model\": \"Mustang\",\n",
        "  \"year\": 1964\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkBZ6gCANMrL"
      },
      "outputs": [],
      "source": [
        "print(thisdict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpnWMsHuKxL5"
      },
      "source": [
        "You can obtain the keys as a standalone list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtrlqssuK3Il"
      },
      "outputs": [],
      "source": [
        "thisdict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoyoyS9qK68-"
      },
      "source": [
        "And the same for the values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGy3_9SBK92h"
      },
      "outputs": [],
      "source": [
        "thisdict.values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LY_zLsR10_7"
      },
      "source": [
        "You can iterate over the keys and values of dictionaries as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcamhP911V--"
      },
      "outputs": [],
      "source": [
        "for key, value in thisdict.items():\n",
        "    print(key + \" \" + str(value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3zo9OcfBIsO"
      },
      "source": [
        "One useful additional thing to consider is that there are different kinds of dictionaries in the Collections library. We will make use of one special kind of dictionary - the default dictionary which returns a default value when asked for a missing key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InlBGl8n9nGy"
      },
      "source": [
        "### The Shannon game\n",
        "\n",
        "Shannon (1951) described an ingenious way of estimating the entropy of English by using human predictions.\n",
        "\n",
        "Shannon, C. E. (1951). Prediction and entropy of printed English. Bell system technical journal, 30(1), 50-64.\n",
        "\n",
        "To play the Shannon game in your own time:\n",
        "\n",
        "Copy this URL into a browser window and press return\n",
        "\n",
        "https://github.com/lianghuang3/shannon_game/archive/refs/heads/main.zip\n",
        "\n",
        "Unpack repository to somewhere on local machine\n",
        "\n",
        "If you are on University Computer or have VS Code installed\n",
        "Open VSCode (e.g. via Anaconda Navigator)\n",
        "Select File -> Open Folder and then select the shannon_game folder\n",
        "\n",
        "Open shannon_game.py and press Run\n",
        "\n",
        "Follow instructions given by the program once running."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGwfrYchCMDb"
      },
      "source": [
        "### Calculate Entropy of English from a sample text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H4vb1HXphOEf",
        "outputId": "f2e4f79e-c6a2-44e5-c875-b73023c153a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-13 13:20:43--  https://www.gutenberg.org/files/2554/2554-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1159923 (1.1M) [text/plain]\n",
            "Saving to: ‘2554-0.txt’\n",
            "\n",
            "2554-0.txt          100%[===================>]   1.11M  1.93MB/s    in 0.6s    \n",
            "\n",
            "2025-10-13 13:20:44 (1.93 MB/s) - ‘2554-0.txt’ saved [1159923/1159923]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "# download from from the internt\n",
        "!wget https://www.gutenberg.org/files/2554/2554-0.txt\n",
        "# read in the file\n",
        "f = open('2554-0.txt')\n",
        "c_and_p = f.read()\n",
        "# select the first chapter - possible because I determined range\n",
        "chapter_one = c_and_p[5464:23725]\n",
        "# convert text to lower case\n",
        "chapter_one=chapter_one.lower()\n",
        "# replace periods and commas with spaces\n",
        "# remove all characters except from a-z and space\n",
        "chapter_one=re.sub('[^a-z ]','', chapter_one)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1a (optional): Use regular expressions to improve tokenisation"
      ],
      "metadata": {
        "id": "PXyHygsSvYZp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0pjZJj5Ynpcn",
        "outputId": "228d57ac-2453-4fe9-c6da-227e14c14361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'some time past he had been in an overstrained irritable conditionverging on hypochondria he had become so completely absorbed inhimself and isolated from his fellows that he dreaded meeting notonly his landlady but anyone at all he was crushed by poverty but theanxieties of his position had of late ceased to weigh upon him he hadgiven up attending to matters of practical importance he had lost alldesire to do so nothing that any landlady could do had a real terrorfor him but to be stopped on the stairs to be forced to listen to hertrivial irrelevant gossip to pestering demands for payment threatsand complaints and to rack his brains for excuses to prevaricate tolieno rather than that he would creep down the stairs like a cat andslip out unseenthis evening however on coming out into the street he became acutelyaware of his fearsi want to attempt a thing like that and am frightened by thesetrifles he thought with an odd smile hm yes all is in a manshands and he lets it all slip from cowardice thats an axiom it wouldbe interesting to know what it is men are most afraid of taking a newstep uttering a new word is what they fear most but i am talkingtoo much its because i chatter that i do nothing or perhaps it isthat i chatter because i do nothing ive learned to chatter thislast month lying for days together in my den thinking of jack thegiantkiller why am i going there now am i capable of that isthat serious it is not serious at all its simply a fantasy to amusemyself a plaything yes maybe it is a playthingthe heat in the street was terrible and the airlessness the bustleand the plaster scaffolding bricks and dust all about him and thatspecial petersburg stench so familiar to all who are unable to get outof town in summerall worked painfully upon the young mans alreadyoverwrought nerves the insufferable stench from the pothouses whichare particularly numerous in that part of the town and the drunken menwhom he met continually although it was a working day completedthe revolting misery of the picture an expression of the profoundestdisgust gleamed for a moment in the young mans refined face he wasby the way exceptionally handsome above the average in height slimwellbuilt with beautiful dark eyes and dark brown hair soon he sankinto deep thought or more accurately speaking into a complete blanknessof mind he walked along not observing what was about him and not caringto observe it from time to time he would mutter something from thehabit of talking to himself to which he had just confessed at thesemoments he would become conscious that his ideas were sometimes in atangle and that he was very weak for two days he had scarcely tastedfoodhe was so badly dressed that even a man accustomed to shabbiness wouldhave been ashamed to be seen in the street in such rags in that quarterof the town however scarcely any shortcoming in dress would havecreated surprise owing to the proximity of the hay market the numberof establishments of bad character the preponderance of the tradingand working class population crowded in these streets and alleys in theheart of petersburg types so various were to be seen in the streetsthat no figure however queer would have caused surprise but there wassuch accumulated bitterness and contempt in the young mans heart thatin spite of all the fastidiousness of youth he minded his rags leastof all in the street it was a different matter when he met withacquaintances or with former fellow students whom indeed he dislikedmeeting at any time and yet when a drunken man who for some unknownreason was being taken somewhere in a huge waggon dragged by a heavydray horse suddenly shouted at him as he drove past hey there germanhatter bawling at the top of his voice and pointing at himthe youngman stopped suddenly and clutched tremulously at his hat it was a tallround hat from zimmermans but completely worn out rusty with age alltorn and bespattered brimless and bent on one side in a most unseemlyfashion not shame however but quite another feeling akin to terrorhad overtaken himi knew it he muttered in confusion i thought so thats the worstof all why a stupid thing like this the most trivial detail mightspoil the whole plan yes my hat is too noticeable it looks absurdand that makes it noticeable with my rags i ought to wear a cap anysort of old pancake but not this grotesque thing nobody wears sucha hat it would be noticed a mile off it would be remembered whatmatters is that people would remember it and that would give thema clue for this business one should be as little conspicuous aspossible trifles trifles are what matter why its just suchtrifles that always ruin everythinghe had not far to go he knew indeed how many steps it was from the gateof his lodging house exactly seven hundred and thirty he had countedthem once when he had been lost in dreams at the time he had put nofaith in those dreams and was only tantalising himself by their hideousbut daring recklessness now a month later he had begun to look uponthem differently and in spite of the monologues in which he jeered athis own impotence and indecision he had involuntarily come to regardthis hideous dream as an exploit to be attempted although hestill did not realise this himself he was positively going now for arehearsal of his project and at every step his excitement grew moreand more violentwith a sinking heart and a nervous tremor he went up to a huge housewhich on one side looked on to the canal and on the other into thestreet this house was let out in tiny tenements and was inhabited byworking people of all kindstailors locksmiths cooks germans ofsorts girls picking up a living as best they could petty clerks etcthere was a continual coming and going through the two gates and in thetwo courtyards of the house three or four doorkeepers were employed onthe building the young man was very glad to meet none of them andat once slipped unnoticed through the door on the right and up thestaircase it was a back staircase dark and narrow but he was familiarwith it already and knew his way and he liked all these surroundingsin such darkness even the most inquisitive eyes were not to be dreadedif i am so scared now what would it be if it somehow came to pass thati were really going to do it he could not help asking himself as hereached the fourth storey there his progress was barred by some porterswho were engaged in moving furniture out of a flat he knew that theflat had been occupied by a german clerk in the civil service and hisfamily this german was moving out then and so the fourth floor on thisstaircase would be untenanted except by the old woman thats a goodthing anyway he thought to himself as he rang the bell of the oldwomans flat the bell gave a faint tinkle as though it were made oftin and not of copper the little flats in such houses always have bellsthat ring like that he had forgotten the note of that bell and nowits peculiar tinkle seemed to remind him of something and to bring itclearly before him he started his nerves were terribly overstrainedby now in a little while the door was opened a tiny crack the oldwoman eyed her visitor with evident distrust through the crack andnothing could be seen but her little eyes glittering in the darknessbut seeing a number of people on the landing she grew bolder andopened the door wide the young man stepped into the dark entry whichwas partitioned off from the tiny kitchen the old woman stood facinghim in silence and looking inquiringly at him she was a diminutivewithered up old woman of sixty with sharp malignant eyes and a sharplittle nose her colourless somewhat grizzled hair was thickly smearedwith oil and she wore no kerchief over it round her thin long neckwhich looked like a hens leg was knotted some sort of flannel ragand in spite of the heat there hung flapping on her shoulders a mangyfur cape yellow with age the old woman coughed and groaned at everyinstant the young man must have looked at her with a rather peculiarexpression for a gleam of mistrust came into her eyes againraskolnikov a student i came here a month ago the young man madehaste to mutter with a half bow remembering that he ought to be morepolitei remember my good sir i remember quite well your coming here theold woman said distinctly still keeping her inquiring eyes on his faceand here i am again on the same errand raskolnikov continued alittle disconcerted and surprised at the old womans mistrust perhapsshe is always like that though only i did not notice it the othertime he thought with an uneasy feelingthe old woman paused as though hesitating then stepped on one sideand pointing to the door of the room she said letting her visitor passin front of herstep in my good sirthe little room into which the young man walked with yellow paper onthe walls geraniums and muslin curtains in the windows was brightlylighted up at that moment by the setting sunso the sun will shine like this then too flashed as it were bychance through raskolnikovs mind and with a rapid glance he scannedeverything in the room trying as far as possible to notice andremember its arrangement but there was nothing special in the room thefurniture all very old and of yellow wood consisted of a sofa witha huge bent wooden back an oval table in front of the sofa adressingtable with a lookingglass fixed on it between the windowschairs along the walls and two or three halfpenny prints in yellowframes representing german damsels with birds in their handsthat wasall in the corner a light was burning before a small ikon everythingwas very clean the floor and the furniture were brightly polishedeverything shonelizavetas work thought the young man there was not a speck of dustto be seen in the whole flatits in the houses of spiteful old widows that one finds suchcleanliness raskolnikov thought again and he stole a curious glanceat the cotton curtain over the door leading into another tiny room inwhich stood the old womans bed and chest of drawers and into which hehad never looked before these two rooms made up the whole flatwhat do you want the old woman said severely coming into the roomand as before standing in front of him so as to look him straight inthe faceive brought something to pawn here and he drew out of his pocketan oldfashioned flat silver watch on the back of which was engraved aglobe the chain was of steelbut the time is up for your last pledge the month was up the daybefore yesterdayi will bring you the interest for another month wait a littlebut thats for me to do as i please my good sir to wait or to sellyour pledge at oncehow much will you give me for the watch alyona ivanovnayou come with such trifles my good sir its scarcely worth anythingi gave you two roubles last time for your ring and one could buy itquite new at a jewelers for a rouble and a halfgive me four roubles for it i shall redeem it it was my fathers ishall be getting some money soona rouble and a half and interest in advance if you likea rouble and a half cried the young manplease yourselfand the old woman handed him back the watch theyoung man took it and was so angry that he was on the point of goingaway but checked himself at once remembering that there was nowhereelse he could go and that he had had another object also in cominghand it over he said roughlythe old woman fumbled in her pocket for her keys and disappeared behindthe curtain into the other room the young man left standing alone inthe middle of the room listened inquisitively thinking he could hearher unlocking the chest of drawersit must be the top drawer he reflected so she carries the keys ina pocket on the right all in one bunch on a steel ring and theresone key there three times as big as all the others with deep notchesthat cant be the key of the chest of drawers then there must be someother chest or strongbox thats worth knowing strongboxes alwayshave keys like that but how degrading it all isthe old woman came backhere sir as we say ten copecks the rouble a month so i must takefifteen copecks from a rouble and a half for the month in advance butfor the two roubles i lent you before you owe me now twenty copeckson the same reckoning in advance that makes thirtyfive copecksaltogether so i must give you a rouble and fifteen copecks for thewatch here it iswhat only a rouble and fifteen copecks nowjust sothe young man did not dispute it and took the money he looked at theold woman and was in no hurry to get away as though there was stillsomething he wanted to say or to do but he did not himself quite knowwhati may be bringing you something else in a day or two alyonaivanovnaa valuable thingsilvera cigarettebox as soon as i get itback from a friend he broke off in confusionwell we will talk about it then sirgoodbyeare you always at home alone your sister is not here withyou he asked her as casually as possible as he went out into thepassagewhat business is she of yours my good siroh nothing particular i simply asked you are too quick gooddayalyona ivanovnaraskolnikov went out in complete confusion this confusion became moreand more intense as he went down the stairs he even stopped short twoor three times as though suddenly struck by some thought when he wasin the street he cried out oh god how loathsome it all is andcan i can i possibly no its nonsense its rubbish he addedresolutely and how could such an atrocious thing come into my headwhat filthy things my heart is capable of yes filthy above alldisgusting loathsome loathsomeand for a whole month ive been but no words no exclamations could express his agitation the feelingof intense repulsion which had begun to oppress and torture his heartwhile he was on his way to the old woman had by now reached such apitch and had taken such a definite form that he did not know what todo with himself to escape from his wretchedness he walked along thepavement like a drunken man regardless of the passersby and jostlingagainst them and only came to his senses when he was in the nextstreet looking round he noticed that he was standing close to a tavernwhich was entered by steps leading from the pavement to the basementat that instant two drunken men came out at the door and abusing andsupporting one another they mounted the steps without stopping tothink raskolnikov went down the steps at once till that moment he hadnever been into a tavern but now he felt giddy and was tormented by aburning thirst he longed for a drink of cold beer and attributed hissudden weakness to the want of food he sat down at a sticky littletable in a dark and dirty corner ordered some beer and eagerly drankoff the first glassful at once he felt easier and his thoughts becameclearall thats nonsense he said hopefully and there is nothing in itall to worry about its simply physical derangement just a glass ofbeer a piece of dry breadand in one moment the brain is strongerthe mind is clearer and the will is firm phew how utterly petty it allisbut in spite of this scornful reflection he was by now looking cheerfulas though he were suddenly set free from a terrible burden and he gazedround in a friendly way at the people in the room but even at thatmoment he had a dim foreboding that this happier frame of mind was alsonot normalthere were few people at the time in the tavern besides the two drunkenmen he had met on the steps a group consisting of about five men anda girl with a concertina had gone out at the same time their departureleft the room quiet and rather empty the persons still in the tavernwere a man who appeared to be an artisan drunk but not extremely sositting before a pot of beer and his companion a huge stout man witha grey beard in a short fullskirted coat he was very drunk and haddropped asleep on the bench every now and then he began as though inhis sleep cracking his fingers with his arms wide apart and the upperpart of his body bounding about on the bench while he hummed somemeaningless refrain trying to recall some such lines as these   his wife a year he fondly loved    his wife aa year hefondly lovedor suddenly waking up again   walking along the crowded row    he met the one he used to knowbut no one shared his enjoyment his silent companion looked withpositive hostility and mistrust at all these manifestations there wasanother man in the room who looked somewhat like a retired governmentclerk he was sitting apart now and then sipping from his pot andlooking round at the company he too appeared to be in some agitationchapter iiraskolnikov was not used to crowds and as we said before he avoidedsociety of every sort more especially of late but now all at once hefelt a desire to be with other people something new seemed to be takingplace within him and with it he felt a sort of thirst for company hewas so weary after a whole month of concentrated wretchedness and gloomyexcitement that he longed to rest if only for a moment in some otherworld whatever it might be and in spite of the filthiness of thesurroundings he was glad now to stay in the tavernthe master of the establishment was in another room but he frequentlycame down some steps into the main room his jaunty tarred boots withred turnover tops coming into view each time before the rest of hisperson he wore a full coa'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "chapter_one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irvJ9z_xIsXj"
      },
      "source": [
        "We are going to be assigning probabilities to sentences using bigram models, so we need to extract unigram and bigram counts. We will store them in dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f4LOgpkKokBv"
      },
      "outputs": [],
      "source": [
        "# First we will build a dictionary of unigram counts\n",
        "# We want to use a special kind of Dictionary called a default dictionary, so we have to import this\n",
        "from collections import defaultdict\n",
        "# Determine the total number of tokens (characters in this case) in the text\n",
        "total_unigrams = len(chapter_one)\n",
        "# Create an empty default dictionary\n",
        "unigrams = defaultdict(int)\n",
        "# Iterate through values of i from zero to the length of the text\n",
        "for i in range(total_unigrams):\n",
        "    # For each element in the list (characters in the chapter), increase the count of that word in our dictionary by one\n",
        "    unigrams[chapter_one[i]] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "afbjVTZRL4VY",
        "outputId": "9741461c-0857-4fed-f475-13e9d9bedf33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "unigrams\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unigrams = sorted(unigrams.keys())\n",
        "for i in unigrams:\n",
        "  print (i)"
      ],
      "metadata": {
        "id": "wcCa4_N6xOcS",
        "outputId": "e3479af7-d123-405e-d075-547155b93bad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'keys'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3445959327.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munigrams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munigrams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unigrams['s']/total_unigrams\n"
      ],
      "metadata": {
        "id": "Pl1D1v5nvtts",
        "outputId": "da02e07c-d687-4224-9a46-aa12427a7a0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05150089995935667"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AskxxL7Ol_uK"
      },
      "outputs": [],
      "source": [
        "# Next we will build a dictionary of bigram counts\n",
        "# We want to use a special kind of Dictionary called a default dictionary, so we have to import this\n",
        "from collections import defaultdict\n",
        "# Determine the total number of bigram tokens (character bigrams in this case) in the text. This is the number of words minus 1\n",
        "total_bigrams = len(chapter_one) - 1\n",
        "# Create an empty default dictionary\n",
        "bigrams = defaultdict(int)\n",
        "# Iterate through values of i from zero to the length of the text minus 1\n",
        "for i in range(total_bigrams):\n",
        "     # For each element in the list (characters in the chapter) extract a bigram consisting of that element and the next and increase the count of that bigram in our dictionary by one\n",
        "    bigrams[chapter_one[i:i+2]] += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m-YWYNIlxql8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4eU0xzDMnH9"
      },
      "outputs": [],
      "source": [
        "bigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_fuwg3WMtrb"
      },
      "source": [
        "Now we want to use these counts to calculate the entropy of English.\n",
        "\n",
        "If we were to calculate the entropy assuming every character was independent and occurred an equal number of times, the calculation would be very simple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFi77r3AswJY"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "-math.log(1/27,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlbIANQjNC7Y"
      },
      "source": [
        "We know that letters do not occur the same number of times (are not equiprobable) so the next thing we might try is to calculate the entropy reflecting the unequal rates but still assuming independence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI8_4dp1slMd"
      },
      "outputs": [],
      "source": [
        "# import the math library so that we can use the math.log function\n",
        "import math\n",
        "# initialise entropy to be zero\n",
        "H=0.0\n",
        "# Iterate over all key value pairs in our unigram count dictionary\n",
        "for key, value in unigrams.items():\n",
        "  # Calculate the unigram surprisal of each letter and add it to the entropy weighting by it relative frequency\n",
        "  H += -(value/total_unigrams * math.log(value/total_unigrams, 2))\n",
        "H"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwsasxmoNWgi"
      },
      "source": [
        "We know that characters are not independent - for each h is more likely to occur following t than following x. This is why we use bigram probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWM5gYb8oWr5"
      },
      "outputs": [],
      "source": [
        "# import the math library so that we can use the math.log function\n",
        "import math\n",
        "# initialise entropy to be zero\n",
        "H=0.0\n",
        "# Iterate over all key value pairs in our unigram count dictionary\n",
        "for key, value in bigrams.items():\n",
        "  # Identity the first element in the bigram\n",
        "  unikey = key[:1]\n",
        "  # Calculate the bigram surprisal for each bigram and add it to the entropy weighting by it relative frequency\n",
        "  H += -(value/total_bigrams * math.log(value/unigrams[unikey], 2))\n",
        "H"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT1xfkYGNjUZ"
      },
      "source": [
        "The estimate is still higher than that we saw in the Shannon game. Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iL4-2j9NrId"
      },
      "source": [
        "Problem 1b: Estimate entropy from a trigram character model. First of all you will need to rewrite the code to extract trigram counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C65BEbi5N4fU"
      },
      "outputs": [],
      "source": [
        "# We want to use a special kind of Dictionary called a default dictionary, so we have to import this\n",
        "from collections import defaultdict\n",
        "# Determine the total number of trigram tokens (character trigrams in this case) in the text.\n",
        "total_trigrams = ???\n",
        "# Create an empty default dictionary\n",
        "trigrams = defaultdict(int)\n",
        "# Iterate through values of i from zero to ??\n",
        "for i in range(total_trigrams):\n",
        "     # For each element in the list (characters in the chapter) extract a trigram consisting of that element and the next 2 and increase the count of that bigram in our dictionary by one\n",
        "    trigrams[chapter_one[????]] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcmRZAvxOgnj"
      },
      "source": [
        "Next you will need to use these to calculate the trigram probabilities and combine to calculate the entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9uTPRxoOsjA"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "H=0.0\n",
        "for key, value in trigrams.items():\n",
        "  bigramkey = ????\n",
        "  H += -(value/??? * math.log(value/???, 2))\n",
        "H"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyizHKmuUuoe"
      },
      "source": [
        "As well as using the counts to estimate the global Entropy of English we can also calculate the log probability of individual sentences including those that weren't in the text from which we estimated our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3Bip0mItzyO"
      },
      "outputs": [],
      "source": [
        "# Define our sentence (a list of characters)\n",
        "sentence3=\" the cat sat on the mat\"\n",
        "# Initialise our log probability\n",
        "log_prob=0.0\n",
        "prob=1.0\n",
        "# Iterate through the sentence from 0 to the length of the sentence minus 1 (because working with bigrams)\n",
        "for i in range(len(sentence3)-1):\n",
        "  #Extract the current bigram of the input sentence\n",
        "  key = sentence3[i:i+2]\n",
        "  # Identity the first element in the bigram\n",
        "  unigram = key[:1]\n",
        "  # Calculate the log bigram probability for the bigram and add it to the log bigram probability for the sentence\n",
        "  log_prob += -math.log(bigrams[key]/unigrams[unigram],2)\n",
        "  prob *= bigrams[key]/unigrams[unigram]\n",
        "log_prob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzEnusR0d-X7"
      },
      "source": [
        "Because we are adding surprisal for each new element in the sentence, the result is length dependent. For general purpose language model evaluation we need to adjust for length, giving us a measure called perplexity. More information on this here:\n",
        "\n",
        "https://githubtocolab.com/cbannard/lela60331_25-26/blob/main/Perplexity.ipynb\n",
        "\n",
        "For now, just note that it is calculated by raising 2 to the log probability of the sentence divided by its length:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pow(2,log_prob/(len(sentence3)-1))"
      ],
      "metadata": {
        "id": "han-nKWhX3KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perplexity has an intuitive interpretation - a perplexity of k means that you are surprised on average as you would be if you'd had to guess between k equiprobable choices."
      ],
      "metadata": {
        "id": "2FtglOQT04BH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5r2qMzYxoG2"
      },
      "source": [
        "### Language modelling for words\n",
        "\n",
        "We are now going to switch to building and exploiting word-based language models. In order to do this we are going to have generate a list of words instead of a list of characters. We are going to do this using re.split()\n",
        "\n",
        "### re.split()\n",
        "re.split() takes a regular expression as a first argument (if you don't have a precompiled pattern) and string as second (or first if you have a precompiled pattern) argument, and splits the string into tokens divided by all substrings matched by the regular expression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToMjvvtfxrIA"
      },
      "outputs": [],
      "source": [
        "# download from from the internt\n",
        "!wget https://www.gutenberg.org/files/2554/2554-0.txt\n",
        "# read in the file\n",
        "f = open('2554-0.txt')\n",
        "c_and_p = f.read()\n",
        "# select the first chapter - possible because I determined range\n",
        "chapter_one = c_and_p[5464:23725]\n",
        "# convert text to lower case\n",
        "chapter_one=chapter_one.lower()\n",
        "chapter_one=re.sub('\\\\n',' ', chapter_one)\n",
        "chapter_one=re.sub(\"\\\\. \",\" eol \", chapter_one)\n",
        "chapter_one=re.sub('[^a-z ]','', chapter_one)\n",
        "chapter_one=re.sub(' +', ' ',chapter_one)\n",
        "chapter_one=re.split(\" \", chapter_one)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkHx5PhgWbaD"
      },
      "source": [
        "With the text in this new tokenised format, the same algorithm can be applied to extract unigram and bigram counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy5066AoyvRT"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "total_unigrams = len(chapter_one) - 1\n",
        "unigrams = defaultdict(int)\n",
        "for i in range(total_unigrams):\n",
        "    unigrams[chapter_one[i]] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQOwFmDNy0QO"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "total_bigrams = len(chapter_one) - 2\n",
        "bigrams = defaultdict(int)\n",
        "for i in range(total_bigrams):\n",
        "    bigrams[str.join(\" \",chapter_one[i:i+2])] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZiLuezcWzIR"
      },
      "outputs": [],
      "source": [
        "unigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzFoLndPWz8m"
      },
      "outputs": [],
      "source": [
        "bigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-eQ9svvW1Oy"
      },
      "source": [
        "And the counts can be used to assign probabilities to sentences in exactly the same way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcLB9nfCzqPU"
      },
      "outputs": [],
      "source": [
        "sentence3=[\"a\",\"man\",\"was\",\"in\",\"the\",\"house\"]\n",
        "log_prob=0.0\n",
        "for i in range(len(sentence3)-1):\n",
        "  key = str.join(\" \",sentence3[i:i+2])\n",
        "  unigram = sentence3[i]\n",
        "  log_prob += -math.log(bigrams[key]/unigrams[unigram],2)\n",
        "log_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prLMG1W-Qh2t"
      },
      "source": [
        "Problem 2: Because we are using bigrams the first element for which we calculate the probability for is the second word. However to give the probability for the full sentence we want to also consider the first word. Update the code so that it does this. You may want to make reference to the lecture slides for the week in order to remind yourself of how we do this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0MJt9ia6Oh_"
      },
      "source": [
        "Now try calculating the log probability for a different sentence of your own creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia4vrQw86YM7"
      },
      "outputs": [],
      "source": [
        "sentence3=[]\n",
        "log_prob=0.0\n",
        "for i in range(len(sentence3)-1):\n",
        "  key = str.join(\" \",sentence3[i:i+2])\n",
        "  unigram = sentence3[i]\n",
        "  log_prob += -math.log(bigrams[key]/unigrams[unigram],2)\n",
        "log_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycwY-IA36Y2a"
      },
      "source": [
        "This probably won't work because of unseen bigrams.\n",
        "\n",
        "We need to solve this problem via smoothing.\n",
        "\n",
        "For example, add-one smoothing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMhPLL_U5PF5"
      },
      "outputs": [],
      "source": [
        "sentence3=[\"a\",\"man\",\"was\",\"in\",\"the\",\"house\",\"when\",\"the\",\"day\",\"arrived\"]\n",
        "log_prob=0.0\n",
        "for i in range(len(sentence3)-1):\n",
        "  key = str.join(\" \",sentence3[i:i+2])\n",
        "  unigram = sentence3[i]\n",
        "  # We add a count of one to each bigram and then add the vocabulary size (number of unique words) to the denominator\n",
        "  log_prob += -math.log((bigrams[key]+1)/(unigrams[unigram]+len(unigrams.keys())),2)\n",
        "log_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCCDnDh68ZtS"
      },
      "source": [
        "Or alternatively, back off smoothing with interpolation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9a7sTCL_8yu"
      },
      "outputs": [],
      "source": [
        "sentence3=[\"a\",\"man\",\"was\",\"in\",\"the\",\"house\",\"when\",\"the\",\"day\",\"arrived\"]\n",
        "log_prob=0.0\n",
        "# These lambdas can change but should sum to 1\n",
        "lambda1 = 0.5\n",
        "lambda2 = 1 - lambda1\n",
        "\n",
        "for i in range(len(sentence3)-1):\n",
        "  key = str.join(\" \",sentence3[i:i+2])\n",
        "  unigram = sentence3[i]\n",
        "  # We combine the unigram and the bigram probabilities, weighting each equally.\n",
        "  log_prob += -math.log((bigrams[key]/unigrams[unigram])*lambda1 + (unigrams[unigram]/total_unigrams)*lambda2,2)\n",
        "log_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77IdH9nXXFbV"
      },
      "source": [
        "Problem 3 (to do in your own time):\n",
        "\n",
        "Estimate a trigram word-based language model. This will require smoothing and you can employ both kinds. You should make use of the code you wrote for character-based trigram language models and the examples of smoothing above."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating with language models\n",
        "\n",
        "Generation with language models is also sometimes called auto-regressive generation. It works by selecting and then outputting words from the vocabulary based on their probability given a preceding context - either (at time point 1) a prompt from the user, or (at subsequent time points) the prompt plus the words that have been generated so far.\n",
        "\n",
        "There are however a wide range of ways in which they are chosen - different methods of what is know as \"decoding\".\n",
        "\n",
        "While real-world language models generate words based on neural language models (e.g. transformers) we can separate the underlying model from the decoding process."
      ],
      "metadata": {
        "id": "H2qojCTJsUgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "nested_dict = lambda: defaultdict(nested_dict)\n",
        "d = nested_dict()\n",
        "\n",
        "for bg in bigrams:\n",
        "  ug = bg.split()\n",
        "  d[ug[0]][ug[1]] = np.log(bigrams[bg]/unigrams[ug[0]])\n",
        "\n",
        "lm=pd.DataFrame(d)"
      ],
      "metadata": {
        "id": "KbZrNey_tTuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generation by Greedy search"
      ],
      "metadata": {
        "id": "Gypxln9Hs48X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define starting word\n",
        "w=\"he\"\n",
        "# Define stopping point - here when an end of line character is output or length reaches 15\n",
        "length = 0\n",
        "while w != \"eol\" and length <= 15:\n",
        "  length += 1\n",
        "  print(w,end=' ')\n",
        "  # get probabilities for all words following the previous word\n",
        "  s=lm[w]\n",
        "\n",
        "  # sort the probabilities and output the most likely word\n",
        "  w=s.sort_values(ascending=False).index[0]\n"
      ],
      "metadata": {
        "id": "aIvM_8hLs0y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generation by sampling"
      ],
      "metadata": {
        "id": "3MmaNVpbs76R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Specify starting word\n",
        "w=\"he\"\n",
        "length=0\n",
        "# Define stopping point - here when an end of line character is output or length reaches 15 words\n",
        "while w != \"eol\" and length <= 15:\n",
        "  length += 1\n",
        "  print(w,end=' ')\n",
        "  # get probabilities for all words following the previous word\n",
        "  s=lm.loc[w]\n",
        "  s=s.drop(s[np.isnan(s)].index)\n",
        "  # Choose randomly from the probability distribution over next words\n",
        "  w=np.random.choice(list(s.index),1,list(np.exp(s.values)))[0]"
      ],
      "metadata": {
        "id": "AvX_wl7KtD2m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}